<div align="center">

# ğŸ¤– Agentic RAG System

### Production-Ready Retrieval-Augmented Generation with LangChain, FastAPI & PGVector

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.115+-green.svg)](https://fastapi.tiangolo.com/)
[![LangChain](https://img.shields.io/badge/LangChain-0.3+-orange.svg)](https://python.langchain.com/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-16+-blue.svg)](https://www.postgresql.org/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Docker](https://img.shields.io/badge/Docker-Ready-2496ED.svg)](https://www.docker.com/)

[Features](#-features) â€¢ [Quick Start](#-quick-start) â€¢ [Architecture](#ï¸-architecture) â€¢ [API Docs](#-api-endpoints) â€¢ [Contributing](#-contributing)

---

</div>

A production-ready Retrieval-Augmented Generation (RAG) system built with LangChain, FastAPI, PostgreSQL with PGVector, and support for both OpenAI and Ollama models.

## ğŸ—ï¸ Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Frontend Layer                            â”‚
â”‚         Streamlit UI  +  OpenWebUI                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ HTTP/REST
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Backend Layer (FastAPI)                      â”‚
â”‚  â€¢ Document Processing (Docling + Semantic Chunking)        â”‚
â”‚  â€¢ Embeddings (OpenAI + Ollama)                             â”‚
â”‚  â€¢ Vector Search (PostgreSQL + PGVector)                    â”‚
â”‚  â€¢ LLM Generation (OpenAI GPT-4 + Ollama Llama3.2)         â”‚
â”‚  â€¢ Conversation Memory                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Data Layer (PostgreSQL + PGVector)              â”‚
â”‚  â€¢ Vector Embeddings Storage                                â”‚
â”‚  â€¢ Document Metadata                                        â”‚
â”‚  â€¢ Conversation History                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Features

- âœ… **Dual Embedding Support**: OpenAI (text-embedding-3-small) OR Ollama (llama3.2)
- âœ… **Dual LLM Support**: OpenAI (gpt-4o-mini) OR Ollama (llama3.2)
- âœ… **Semantic Chunking**: Intelligent document splitting using percentile-based breakpoints
- âœ… **Vector Search**: PostgreSQL with PGVector extension for similarity search
- âœ… **Document Processing**: Support for PDF, DOCX, TXT, MD using Docling
- âœ… **Conversation Memory**: Context-aware chat with history
- âœ… **RESTful API**: FastAPI with automatic OpenAPI documentation
- âœ… **Observability**: Arize Phoenix integration for tracing
- âœ… **Evaluation**: RAGAs for quality metrics

## ğŸ“ Project Structure
```
RAG_LC/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ route_embeddings.py       # Embeddings API
â”‚   â”‚   â”œâ”€â”€ embeddings.py             # Embeddings logic
â”‚   â”‚   â”œâ”€â”€ route_documents.py        # Documents API
â”‚   â”‚   â”œâ”€â”€ documents.py              # Document processing
â”‚   â”‚   â”œâ”€â”€ route_chat.py             # Chat API
â”‚   â”‚   â””â”€â”€ chat.py                   # Chat logic
â”‚   â”‚
â”‚   â”œâ”€â”€ config.py                     # Configuration
â”‚   â”œâ”€â”€ database.py                   # PostgreSQL + PGVector
â”‚   â”œâ”€â”€ models.py                     # Pydantic models
â”‚   â”œâ”€â”€ main.py                       # FastAPI app
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ .env
â”‚
â”œâ”€â”€ uploads/                          # Document uploads
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

## ğŸ› ï¸ Tech Stack

| Component | Technology                           |
|-----------|--------------------------------------|
| **Backend Framework** | FastAPI                              |
| **Document Processing** | Docling                              |
| **Text Chunking** | LangChain SemanticChunker            |
| **Embeddings** | OpenAI + Ollama                      |
| **Vector Database** | PostgreSQL + PGVector                |
| **LLMs** | OpenAI GPT-4o-mini + Ollama Llama3.2 |
| **Orchestration** | LangChain + LangGraph                |
| **Observability** | Arize Phoenix                        |
| **Evaluation** | RAGAS Framework                      |
| **Deployment** | Dockerfile                           |

## ğŸ“‹ Prerequisites

- Python 3.11+
- Docker & Docker Compose
- OpenAI API Key
- UV package manager (optional, recommended)


## ğŸ”§ Configuration

All configuration is managed through environment variables in `.env` file:

- **OpenAI Settings**: API key, models
- **Ollama Settings**: Base URL, models
- **PostgreSQL Settings**: Connection details
- **Vector Store Settings**: Collection names, dimensions
- **RAG Settings**: Chunk size, top-k retrieval

## ğŸ¯ Roadmap

- [x] Configuration management
- [x] Database setup with PGVector
- [x] Embeddings service (OpenAI + Ollama)
- [x] Document processing with Docling
- [ ] Vector store operations
- [ ] LLM service (OpenAI + Ollama)
- [ ] RAG pipeline
- [ ] Chat endpoints
- [ ] Conversation memory
- [ ] Streamlit frontend
- [ ] OpenWebUI integration
- [ ] Arize Phoenix observability
- [ ] RAGAs evaluation framework
- [ ] Document indexer CLI
- [ ] Production deployment

```

## ğŸ“š Documentation

- [LangChain Documentation](https://python.langchain.com/)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [PGVector Documentation](https://github.com/pgvector/pgvector)
- [Ollama Documentation](https://ollama.ai/)
- [Docling Documentation](https://github.com/DS4SD/docling)

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Open a Pull Request

## ğŸ“„ License

MIT License

## ğŸ‘¥ Author

- Mohammed Abdul Azeem Siddiqui

## ğŸ™ Acknowledgments

- LangChain team for the amazing framework
- OpenAI for embeddings and LLMs
- Ollama for local LLM support





A modular Retrieval-Augmented Generation (RAG) application built with:
- Document ingestion via Docling
- Semantic chunking via LangChainâ€™s `SemanticChunker`
- Vector storage using pgvector (PostgreSQL)
- Dual LLM support (OpenAI API & Ollama)
- Observability and cost tracking via Arize Phoenix

---

## âš™ï¸ Project Structure

```

rag_app/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ settings.yaml
â”‚   â””â”€â”€ logging.yaml
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_docs/
â”‚   â”œâ”€â”€ processed_docs/
â”‚   â””â”€â”€ chunks/
â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ docling_loader.py
â”‚   â”œâ”€â”€ chunker.py
â”‚   â””â”€â”€ ingest_service.py
â”œâ”€â”€ embeddings/
â”‚   â”œâ”€â”€ embedder.py
â”‚   â”œâ”€â”€ vector_store.py
â”‚   â””â”€â”€ indexer.py
â”œâ”€â”€ llm/
â”‚   â”œâ”€â”€ llm_base.py
â”‚   â”œâ”€â”€ llm_openai.py
â”‚   â””â”€â”€ llm_ollama.py
â”œâ”€â”€ retrieval/
â”‚   â”œâ”€â”€ retriever.py
â”‚   â”œâ”€â”€ reranker_pointwise.py
â”‚   â””â”€â”€ retrieval_pipeline.py
â”œâ”€â”€ generation/
â”‚   â”œâ”€â”€ prompt_templates/
â”‚   â”œâ”€â”€ generator.py
â”‚   â””â”€â”€ api/
â”‚       â”œâ”€â”€ main.py
â”‚       â”œâ”€â”€ dependencies.py
â”‚       â”œâ”€â”€ schemas.py
â”‚       â””â”€â”€ routers/
â”‚           â”œâ”€â”€ query.py
â”‚           â”œâ”€â”€ ingest.py
â”‚           â””â”€â”€ admin.py
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ streamlit_app.py
â”œâ”€â”€ observability/
â”‚   â”œâ”€â”€ arize_setup.py
â”‚   â”œâ”€â”€ prompt_tracking.py
â”‚   â”œâ”€â”€ retrieval_tracking.py
â”‚   â””â”€â”€ cost_tracker.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â””â”€â”€ integration/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

````

---

## ğŸš€ Installation & Setup

1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/your-repo.git
   cd your-repo
````

2. Create and activate a virtual environment (Python 3.10+ recommended):

   ```bash
   python -m venv venv
   source venv/bin/activate   # on Linux/Mac
   venv\Scripts\activate      # on Windows
   ```

3. Install dependencies:

   ```bash
   pip install -r requirements.txt
   ```

4. Configure `config/settings.yaml` with your keys:

   ```yaml
   openai:
     api_key: "YOUR_OPENAI_API_KEY"
     model: "gpt-4o-turbo"
   ollama:
     host: "http://localhost:11434"
     model: "llama3-4b"
   database:
     postgres_url: "postgresql://user:password@localhost:5432/rag_db"
   ```

## ğŸ§© Usage

### Ingest a document

Either use the Streamlit UI or the API:

```bash
# via API
curl -X POST "http://localhost:8000/api/v1/ingest/upload" \
     -F "file=@path/to/doc.pdf"
```

Or open Streamlit (`ui/streamlit_app.py`) and upload a document.

### Query for an answer

```bash
# via API
curl -X POST "http://localhost:8000/api/v1/query" \
     -H "Content-Type: application/json" \
     -d '{"query":"What are the benefits of RAG?"}'
```

Or use the Streamlit UI: select **Query Documents** tab, type your query, and run.

---

## ğŸ” Features

* **Dual LLM support**: Switch between OpenAI and Ollama via `settings.yaml`.
* **Semantic chunking**: Uses embedding-based chunk splitting for more meaningful chunks.
* **Persistent vector store**: All chunks and embeddings stored in PostgreSQL with pgvector.
* **Observability**: Track prompt usage, retrieval latency, token costs via Arize Phoenix.
* **Modular architecture**: Clear separation of loading, chunking, embedding, retrieval, reranking, generation, UI, and tests.

---

## ğŸ§ª Running Tests

Run the test suite (unit + integration) with pytest:

```bash
pytest
```

---

## ğŸ“† Roadmap

* Support additional embedding backends (e.g., SentenceTransformers).
* Introduce list-wise reranking (in addition to point-wise).
* Add Docker and Kubernetes deployment.
* Enhance Streamlit UI with analytics dashboards (token usage, cost over time).
* Expand documentation (developer guides, architecture diagrams).

---

## ğŸ§‘â€ğŸ’» Contributing

Contributions are welcome!
Please:

* Fork the repository
* Create a branch (`feature/my-feature`)
* Write tests and update documentation
* Submit a pull request

---

## ğŸ“ License

This project is licensed under the MIT License â€“ see `LICENSE` for more details.

---

## ğŸ“« Contact

Maintained by *Your Name*.
Feel free to open issues, submit pull requests, or connect via [your-email@example.com](mailto:your-email@example.com).

```

You can modify the placeholders (GitHub URL, Your Name, contact email) as needed.

If youâ€™d like, I can **generate a basic `LICENSE` file** next (MIT license template) for you.
::contentReference[oaicite:0]{index=0}
```
