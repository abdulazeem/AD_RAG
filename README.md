```markdown
# RAG Pipeline with Docling, LangChain, PGVector & Arize Phoenix

A modular Retrieval-Augmented Generation (RAG) application built with:
- Document ingestion via Docling
- Semantic chunking via LangChainâ€™s `SemanticChunker`
- Vector storage using pgvector (PostgreSQL)
- Dual LLM support (OpenAI API & Ollama)
- Observability and cost tracking via Arize Phoenix

---

## âš™ï¸ Project Structure

```

rag_app/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ settings.yaml
â”‚   â””â”€â”€ logging.yaml
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_docs/
â”‚   â”œâ”€â”€ processed_docs/
â”‚   â””â”€â”€ chunks/
â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ docling_loader.py
â”‚   â”œâ”€â”€ chunker.py
â”‚   â””â”€â”€ ingest_service.py
â”œâ”€â”€ embeddings/
â”‚   â”œâ”€â”€ embedder.py
â”‚   â”œâ”€â”€ vector_store.py
â”‚   â””â”€â”€ indexer.py
â”œâ”€â”€ llm/
â”‚   â”œâ”€â”€ llm_base.py
â”‚   â”œâ”€â”€ llm_openai.py
â”‚   â””â”€â”€ llm_ollama.py
â”œâ”€â”€ retrieval/
â”‚   â”œâ”€â”€ retriever.py
â”‚   â”œâ”€â”€ reranker_pointwise.py
â”‚   â””â”€â”€ retrieval_pipeline.py
â”œâ”€â”€ generation/
â”‚   â”œâ”€â”€ prompt_templates/
â”‚   â”œâ”€â”€ generator.py
â”‚   â””â”€â”€ api/
â”‚       â”œâ”€â”€ main.py
â”‚       â”œâ”€â”€ dependencies.py
â”‚       â”œâ”€â”€ schemas.py
â”‚       â””â”€â”€ routers/
â”‚           â”œâ”€â”€ query.py
â”‚           â”œâ”€â”€ ingest.py
â”‚           â””â”€â”€ admin.py
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ streamlit_app.py
â”œâ”€â”€ observability/
â”‚   â”œâ”€â”€ arize_setup.py
â”‚   â”œâ”€â”€ prompt_tracking.py
â”‚   â”œâ”€â”€ retrieval_tracking.py
â”‚   â””â”€â”€ cost_tracker.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â””â”€â”€ integration/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

````

---

## ğŸš€ Installation & Setup

1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/your-repo.git
   cd your-repo
````

2. Create and activate a virtual environment (Python 3.10+ recommended):

   ```bash
   python -m venv venv
   source venv/bin/activate   # on Linux/Mac
   venv\Scripts\activate      # on Windows
   ```

3. Install dependencies:

   ```bash
   pip install -r requirements.txt
   ```

4. Configure `config/settings.yaml` with your keys:

   ```yaml
   openai:
     api_key: "YOUR_OPENAI_API_KEY"
     model: "gpt-4o-turbo"
   ollama:
     host: "http://localhost:11434"
     model: "llama3-4b"
   database:
     postgres_url: "postgresql://user:password@localhost:5432/rag_db"
   ```

## ğŸ§© Usage

### Ingest a document

Either use the Streamlit UI or the API:

```bash
# via API
curl -X POST "http://localhost:8000/api/v1/ingest/upload" \
     -F "file=@path/to/doc.pdf"
```

Or open Streamlit (`ui/streamlit_app.py`) and upload a document.

### Query for an answer

```bash
# via API
curl -X POST "http://localhost:8000/api/v1/query" \
     -H "Content-Type: application/json" \
     -d '{"query":"What are the benefits of RAG?"}'
```

Or use the Streamlit UI: select **Query Documents** tab, type your query, and run.

---

## ğŸ” Features

* **Dual LLM support**: Switch between OpenAI and Ollama via `settings.yaml`.
* **Semantic chunking**: Uses embedding-based chunk splitting for more meaningful chunks.
* **Persistent vector store**: All chunks and embeddings stored in PostgreSQL with pgvector.
* **Observability**: Track prompt usage, retrieval latency, token costs via Arize Phoenix.
* **Modular architecture**: Clear separation of loading, chunking, embedding, retrieval, reranking, generation, UI, and tests.

---

## ğŸ§ª Running Tests

Run the test suite (unit + integration) with pytest:

```bash
pytest
```

---

## ğŸ“† Roadmap

* Support additional embedding backends (e.g., SentenceTransformers).
* Introduce list-wise reranking (in addition to point-wise).
* Add Docker and Kubernetes deployment.
* Enhance Streamlit UI with analytics dashboards (token usage, cost over time).
* Expand documentation (developer guides, architecture diagrams).

---

## ğŸ§‘â€ğŸ’» Contributing

Contributions are welcome!
Please:

* Fork the repository
* Create a branch (`feature/my-feature`)
* Write tests and update documentation
* Submit a pull request

---

## ğŸ“ License

This project is licensed under the MIT License â€“ see `LICENSE` for more details.

---

## ğŸ“« Contact

Maintained by *Your Name*.
Feel free to open issues, submit pull requests, or connect via [your-email@example.com](mailto:your-email@example.com).

```

You can modify the placeholders (GitHub URL, Your Name, contact email) as needed.

If youâ€™d like, I can **generate a basic `LICENSE` file** next (MIT license template) for you.
::contentReference[oaicite:0]{index=0}
```
