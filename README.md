<div align="center">

# ğŸ¤– Agentic RAG System

### Production-Ready Retrieval-Augmented Generation with LangChain, FastAPI & PGVector

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.120+-green.svg)](https://fastapi.tiangolo.com/)
[![LangChain](https://img.shields.io/badge/LangChain-0.3+-orange.svg)](https://python.langchain.com/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-16+-blue.svg)](https://www.postgresql.org/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.50+-red.svg)](https://streamlit.io/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

[Features](#-features) â€¢ [Quick Start](#-quick-start) â€¢ [Architecture](#ï¸-architecture) â€¢ [API Docs](#-api-endpoints) â€¢ [Integrations](#-integrations)

---

</div>

A production-ready, modular Retrieval-Augmented Generation (RAG) system featuring dual LLM support (OpenAI & Ollama), advanced document processing with Docling, semantic chunking, vector search with PostgreSQL+PGVector, and comprehensive observability through Arize Phoenix.

## ğŸ—ï¸ Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Frontend Layer                            â”‚
â”‚         Streamlit UI  +  OpenWebUI                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ HTTP/REST
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Backend Layer (FastAPI)                      â”‚
â”‚  â€¢ Document Processing (Docling + Semantic Chunking)        â”‚
â”‚  â€¢ Embeddings (OpenAI + Ollama)                             â”‚
â”‚  â€¢ Vector Search (PostgreSQL + PGVector)                    â”‚
â”‚  â€¢ LLM Generation (OpenAI GPT-4 + Ollama Llama3.2)         â”‚
â”‚  â€¢ Conversation Memory                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Data Layer (PostgreSQL + PGVector)              â”‚
â”‚  â€¢ Vector Embeddings Storage                                â”‚
â”‚  â€¢ Document Metadata                                        â”‚
â”‚  â€¢ Conversation History                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Features

### Core Capabilities
- âœ… **Dual Embedding Support**: OpenAI (text-embedding-ada-002) OR Ollama (nomic-embed-text)
- âœ… **Dual LLM Support**: OpenAI (gpt-4o-mini) OR Ollama (llama3.2:latest)
- âœ… **Advanced Document Processing**: Multi-format support (PDF, DOCX, TXT, MD) via Docling
- âœ… **Semantic Chunking**: LangChain SemanticChunker with percentile-based breakpoints
- âœ… **Vector Search**: PostgreSQL with PGVector extension for efficient similarity search
- âœ… **Intelligent Reranking**: Pointwise reranking for improved result relevance

### User Interfaces
- âœ… **Streamlit UI**: Full-featured web interface for document management, chat, and evaluation
- âœ… **Open WebUI Integration**: Custom pipeline for seamless Open WebUI integration
- âœ… **RESTful API**: Comprehensive FastAPI with automatic OpenAPI documentation

### Advanced Features
- âœ… **Conversation Memory**: Multi-session chat with context-aware responses
- âœ… **Evaluation Framework**: Custom evaluation system with ground truth generation and metrics
- âœ… **Observability**: Arize Phoenix integration with prompt management and cost tracking
- âœ… **Admin Tools**: Document management, bulk operations, and system monitoring
- âœ… **Flexible Configuration**: Environment-based settings with backend switching

## ğŸ“ Project Structure
```
RAG_v2/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ settings.py                   # Environment-based configuration
â”‚   â””â”€â”€ backend_config.py             # Backend-specific settings
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_docs/                     # Original uploaded documents
â”‚   â”œâ”€â”€ processed_docs/               # Processed document data
â”‚   â”œâ”€â”€ chunks/                       # Generated chunks
â”‚   â””â”€â”€ evaluation/                   # Evaluation datasets & results
â”œâ”€â”€ database/
â”‚   â”œâ”€â”€ init_db.py                    # Database initialization
â”‚   â””â”€â”€ models.py                     # SQLAlchemy models
â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ docling_loader.py             # Docling document loader
â”‚   â”œâ”€â”€ chunker.py                    # Semantic chunking logic
â”‚   â””â”€â”€ ingest_service.py             # Document ingestion service
â”œâ”€â”€ embeddings/
â”‚   â”œâ”€â”€ embedder.py                   # Embedding generation
â”‚   â”œâ”€â”€ vector_store.py               # PGVector operations
â”‚   â””â”€â”€ indexer.py                    # Document indexing
â”œâ”€â”€ llm/
â”‚   â”œâ”€â”€ llm_base.py                   # LLM base class
â”‚   â”œâ”€â”€ llm_openai.py                 # OpenAI implementation
â”‚   â””â”€â”€ llm_ollama.py                 # Ollama implementation
â”œâ”€â”€ retrieval/
â”‚   â”œâ”€â”€ retriever.py                  # Vector retrieval
â”‚   â”œâ”€â”€ reranker_pointwise.py         # Pointwise reranker
â”‚   â””â”€â”€ retrieval_pipeline.py         # Complete retrieval pipeline
â”œâ”€â”€ generation/
â”‚   â”œâ”€â”€ generator.py                  # Response generation
â”‚   â”œâ”€â”€ prompt_templates/             # Prompt templates
â”‚   â””â”€â”€ api/
â”‚       â”œâ”€â”€ main.py                   # FastAPI application
â”‚       â””â”€â”€ routers/
â”‚           â”œâ”€â”€ query.py              # Query endpoints
â”‚           â”œâ”€â”€ ingest.py             # Ingestion endpoints
â”‚           â”œâ”€â”€ chat.py               # Chat endpoints
â”‚           â”œâ”€â”€ rerank.py             # Reranking endpoints
â”‚           â”œâ”€â”€ evaluation.py         # Evaluation endpoints
â”‚           â””â”€â”€ admin.py              # Admin endpoints
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ ground_truth_generator.py     # Generate evaluation datasets
â”‚   â””â”€â”€ llm_evaluator.py              # LLM-based evaluation
â”œâ”€â”€ observability/
â”‚   â”œâ”€â”€ arize_setup.py                # Phoenix initialization
â”‚   â”œâ”€â”€ phoenix_prompt_manager.py     # Prompt management
â”‚   â”œâ”€â”€ cost_tracker.py               # Cost tracking
â”‚   â”œâ”€â”€ prompt_tracking.py            # Prompt tracking
â”‚   â””â”€â”€ retrieval_tracking.py         # Retrieval tracking
â”œâ”€â”€ open_webui_integration/
â”‚   â”œâ”€â”€ rag_pipeline.py               # Open WebUI pipeline
â”‚   â””â”€â”€ README.md                     # Integration guide
â”œâ”€â”€ streamlit_app.py                  # Streamlit web interface
â”œâ”€â”€ main.py                           # Application entry point
â”œâ”€â”€ requirements.txt                  # Python dependencies
â”œâ”€â”€ pyproject.toml                    # Project metadata (uv)
â”œâ”€â”€ .env.example                      # Example environment variables
â””â”€â”€ README.md                         # This file
```

## ğŸ› ï¸ Tech Stack

| Component | Technology |
|-----------|------------|
| **Backend Framework** | FastAPI 0.120+ |
| **Document Processing** | Docling 2.60+ |
| **Text Chunking** | LangChain SemanticChunker |
| **Embeddings** | OpenAI text-embedding-ada-002 / Ollama nomic-embed-text |
| **Vector Database** | PostgreSQL 16+ with PGVector |
| **LLMs** | OpenAI gpt-4o-mini / Ollama llama3.2:latest |
| **Orchestration** | LangChain 0.3+ |
| **Observability** | Arize Phoenix 12.9+ |
| **Evaluation** | Custom (RAGAs-like) Framework |
| **UI Framework** | Streamlit 1.50+ |
| **Open WebUI** | Custom Pipeline Integration |

## ğŸ“‹ Prerequisites

- **Python 3.11+**
- **PostgreSQL 16+** with PGVector extension
- **OpenAI API Key** (if using OpenAI backend)
- **Ollama** (if using Ollama backend) - [Installation Guide](https://ollama.ai/)
- **Arize Phoenix** (optional, for observability) - `python -m phoenix.server.main serve`
- **UV package manager** (recommended) or pip

## ğŸš€ Quick Start

### 1. Clone and Setup

```bash
# Clone the repository
git clone <your-repo-url>
cd RAG_v2

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
# OR using uv (faster)
uv pip install -r requirements.txt
```

### 2. Setup PostgreSQL with PGVector

```bash
# Using Docker
docker run -d \
  --name postgres-pgvector \
  -e POSTGRES_PASSWORD=postgres \
  -e POSTGRES_DB=rag_db \
  -p 5432:5432 \
  pgvector/pgvector:pg16

# The database will be automatically initialized on first API start
```

### 3. Configure Environment

```bash
# Copy example environment file
cp .env.example .env

# Edit .env with your settings
nano .env  # or use your preferred editor
```

**Key configuration options:**

```bash
# Backend Selection
LLM_BACKEND=openai              # or "ollama"
EMBEDDING_BACKEND=openai        # or "ollama"

# OpenAI Configuration
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-4o-mini

# Ollama Configuration (if using Ollama)
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama3.2:latest

# Database
DATABASE_URL=postgresql://postgres:postgres@localhost:5432/rag_db

# Observability
PHOENIX_COLLECTOR_ENDPOINT=http://localhost:6006
ENABLE_COST_TRACKING=true
```

### 4. Start the Services

#### Option A: FastAPI Backend Only

```bash
# Start the FastAPI backend
uvicorn generation.api.main:app --reload --host 0.0.0.0 --port 8000

# API will be available at http://localhost:8000
# API docs at http://localhost:8000/docs
```

#### Option B: Streamlit UI

```bash
# Start Streamlit interface
streamlit run streamlit_app.py

# UI will be available at http://localhost:8501
```

#### Option C: Both Services

```bash
# Terminal 1: Start API
uvicorn generation.api.main:app --reload --port 8000

# Terminal 2: Start Streamlit
streamlit run streamlit_app.py
```

#### Option D: With Phoenix Observability

```bash
# Terminal 1: Start Phoenix
python -m phoenix.server.main serve

# Terminal 2: Start API
uvicorn generation.api.main:app --reload --port 8000

# Terminal 3: Start Streamlit
streamlit run streamlit_app.py

# Phoenix UI at http://localhost:6006
```

## ğŸ“š API Endpoints

### Document Ingestion
- `POST /api/v1/ingest/` - Upload and process a document
- `POST /api/v1/ingest/bulk` - Bulk upload documents
- `DELETE /api/v1/ingest/{doc_id}` - Delete a document

### Query & Retrieval
- `POST /api/v1/query/` - Query the RAG system
- `POST /api/v1/rerank/` - Rerank retrieved documents

### Chat
- `POST /api/v1/chat/sessions` - Create chat session
- `GET /api/v1/chat/sessions` - List chat sessions
- `POST /api/v1/chat/sessions/{chat_id}/messages` - Send message
- `GET /api/v1/chat/sessions/{chat_id}/messages` - Get chat history
- `DELETE /api/v1/chat/sessions/{chat_id}` - Delete chat session

### Evaluation
- `POST /api/v1/evaluation/generate-ground-truth` - Generate evaluation dataset
- `POST /api/v1/evaluation/evaluate` - Run evaluation
- `GET /api/v1/evaluation/ground-truth-files` - List ground truth files
- `GET /api/v1/evaluation/evaluation-results` - List evaluation results

### Admin
- `GET /api/v1/admin/documents/{backend}` - List documents
- `DELETE /api/v1/admin/documents/{backend}` - Delete all documents
- `GET /api/v1/admin/stats/{backend}` - Get system statistics

**Full API documentation**: http://localhost:8000/docs


### Arize Phoenix Observability

Phoenix provides comprehensive observability for your RAG pipeline:

- **Trace LLM calls** with prompt and response tracking
- **Monitor costs** for OpenAI API usage
- **Track retrieval** performance and relevance
- **Manage prompts** with version control

**Access Phoenix UI**: http://localhost:6006

## ğŸ® Usage Examples

### Via API

```bash
# 1. Upload a document
curl -X POST "http://localhost:8000/api/v1/ingest/" \
  -F "file=@document.pdf"

# 2. Query the document
curl -X POST "http://localhost:8000/api/v1/query/" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What is the main topic?",
    "backend": "openai",
    "top_k": 20,
    "rerank_top_m": 5
  }'

# 3. Start a chat session
curl -X POST "http://localhost:8000/api/v1/chat/sessions" \
  -H "Content-Type: application/json" \
  -d '{"session_name": "My Chat"}'

# 4. Send a message
curl -X POST "http://localhost:8000/api/v1/chat/sessions/{chat_id}/messages" \
  -H "Content-Type: application/json" \
  -d '{"message": "Explain this concept"}'
```

### Via Python

```python
import requests

# Query endpoint
response = requests.post(
    "http://localhost:8000/api/v1/query/",
    json={
        "query": "What are the key features?",
        "backend": "openai",
        "top_k": 20
    }
)
result = response.json()
print(f"Answer: {result['answer']}")
print(f"Sources: {result['source_documents']}")
```

### Via Streamlit UI

1. **Document Management**: Upload, view, and delete documents
2. **Query Interface**: Ask questions with customizable retrieval settings
3. **Chat Interface**: Multi-turn conversations with context
4. **Evaluation**: Generate ground truth and evaluate system performance
5. **Reranking Test**: Test and compare different reranking strategies

## ğŸ”¬ Evaluation

The system includes a custom evaluation framework for assessing RAG performance:

### Generate Ground Truth

```bash
# Via API
curl -X POST "http://localhost:8000/api/v1/evaluation/generate-ground-truth" \
  -H "Content-Type: application/json" \
  -d '{
    "backend": "openai",
    "num_questions": 10,
    "output_filename": "eval_dataset.json"
  }'
```

### Run Evaluation

```bash
# Via API
curl -X POST "http://localhost:8000/api/v1/evaluation/evaluate" \
  -H "Content-Type: application/json" \
  -d '{
    "ground_truth_file": "eval_dataset.json",
    "backend": "openai"
  }'
```



## ğŸ”§ Configuration

All configuration is managed via environment variables in `.env`:

**Application Settings:**
- `APP_NAME`, `APP_VERSION`

**Backend Selection:**
- `LLM_BACKEND` - "openai" or "ollama"
- `EMBEDDING_BACKEND` - "openai" or "ollama"

**OpenAI Settings:**
- `OPENAI_API_KEY`, `OPENAI_MODEL`, `OPENAI_TIMEOUT`

**Ollama Settings:**
- `OLLAMA_HOST`, `OLLAMA_MODEL`, `OLLAMA_TIMEOUT`

**Database Settings:**
- `DATABASE_URL`, `PGVECTOR_TABLE`

**Retrieval Settings:**
- `RETRIEVAL_TOP_K`, `RETRIEVAL_RERANK_TOP_M`
- `RETRIEVAL_CHUNK_SIZE`, `RETRIEVAL_CHUNK_OVERLAP`

**Observability:**
- `PHOENIX_COLLECTOR_ENDPOINT`
- `ENABLE_COST_TRACKING`, `ENABLE_PROMPT_TRACKING`

See `.env.example` for all available options.

## ğŸ“š Documentation

- [LangChain Documentation](https://python.langchain.com/)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [PGVector Documentation](https://github.com/pgvector/pgvector)
- [Ollama Documentation](https://ollama.ai/)
- [Docling Documentation](https://github.com/DS4SD/docling)
- [Arize Phoenix Documentation](https://docs.arize.com/phoenix/)
- [Streamlit Documentation](https://docs.streamlit.io/)



## ğŸ‘¥ Author

- Mohammed Abdul Azeem Siddiqui

## ğŸ™ Acknowledgments

- LangChain team for the amazing framework
- OpenAI for embeddings and LLMs
- Ollama for local LLM support
- Docling team for document processing
- Arize team for Phoenix observability
